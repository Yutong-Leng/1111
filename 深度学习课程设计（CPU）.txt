import numpy as np

# 数据加载
def load_mnist_images(filename):
    with open(filename, 'rb') as f:
        f.read(16)  # 跳过文件头
        data = np.frombuffer(f.read(), dtype=np.uint8)
        data = data.reshape(-1, 1, 28, 28).astype(np.float32) / 255.0  # 归一化
        return data

def load_mnist_labels(filename):
    with open(filename, 'rb') as f:
        f.read(8)  # 跳过文件头
        labels = np.frombuffer(f.read(), dtype=np.uint8)
        return labels

# 独热编码
def one_hot_encode(labels, num_classes=10):
    return np.eye(num_classes)[labels]

# 权重初始化
def initialize_weights(shape, method="xavier"):
    if method == "xavier":
        return np.random.randn(*shape) * np.sqrt(2 / sum(shape))
    elif method == "he":
        return np.random.randn(*shape) * np.sqrt(2 / shape[0])
    elif method == "uniform":
        return np.random.uniform(-0.1, 0.1, shape)
    else:
        raise ValueError("Unknown initialization method")

# 激活函数
def relu(x):
    return np.maximum(0, x.data)  # 从 Tensor 中提取数据

def relu_derivative(x):
    return np.where(x > 0, 1, 0)

def softmax(x):
    # 确保 x 是二维数组，形状为 (batch_size, num_classes)
    if x.data.ndim == 1:
        x_data = x.data.reshape(1, -1)  # 将一维数组转换为二维
    else:
        x_data = x.data
    exp_x = np.exp(x_data - np.max(x_data, axis=-1, keepdims=True))  # 使用 axis=-1 计算每个样本的最大值
    return Tensor(exp_x / np.sum(exp_x, axis=-1, keepdims=True), requires_grad=False)  # 返回一个 Tensor 对象

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# 损失函数
def cross_entropy_loss(y_true, y_pred):
    return -np.sum(y_true * np.log(y_pred + 1e-9)) / y_true.shape[0]

def binary_cross_entropy_loss(y_true, y_pred):
    return -np.mean(y_true * np.log(y_pred + 1e-9) + (1 - y_true) * np.log(1 - y_pred + 1e-9))

# 实现一个简单的Graph类来管理操作
class Graph:
    def __init__(self):
        self.operations = []

    def add_operation(self, operation):
        self.operations.append(operation)

    def compute_gradients(self):
        # 从最后一个操作开始反向传播
        for op in reversed(self.operations):
            op.backward()

    def clear(self):
        self.operations = []

# 定义Tensor类
class Tensor:
    def __init__(self, data, requires_grad=False, graph=None):
        self.data = np.array(data, dtype=np.float32)
        self.requires_grad = requires_grad
        self.grad = np.zeros_like(self.data) if requires_grad else None
        self._backward = lambda: None
        self.graph = graph

    def backward(self):
        if self.requires_grad:
            self._backward()
            if self.graph is not None:
                self.graph.compute_gradients()

# 优化器：SGD
class SGD:
    def __init__(self, learning_rate=0.01, weight_decay=0.0):
        self.learning_rate = learning_rate
        self.weight_decay = weight_decay

    def step(self, parameters, gradients):
        for param, grad in zip(parameters, gradients):
            if self.weight_decay > 0:
                grad += self.weight_decay * param  # Apply weight decay
            param -= self.learning_rate * grad

# 全连接层
class FullyConnected:
    def __init__(self, input_size, output_size, activation_function=None, graph=None):
        self.input_size = input_size
        self.output_size = output_size
        self.activation_function = activation_function
        self.graph = graph

        # Initialize weights and biases
        self.weights = Tensor(np.random.randn(input_size, output_size) * np.sqrt(2. / input_size), requires_grad=True, graph=graph)
        self.bias = Tensor(np.zeros(output_size), requires_grad=True, graph=graph)

    def forward(self, input):
        self.input = input
        self.output = np.dot(input, self.weights.data) + self.bias.data
        if self.activation_function:
            self.output = self.activation_function(self.output)
        return Tensor(self.output, requires_grad=True, graph=self.graph)

    def backward(self, grad_output):
        if self.activation_function:
            grad_output = grad_output * relu_derivative(self.output)

        self.d_weights = np.dot(self.input.T, grad_output)
        self.d_bias = np.sum(grad_output, axis=0)
        grad_input = np.dot(grad_output, self.weights.data.T)

        # 更新梯度
        self.weights.grad = self.d_weights
        self.bias.grad = self.d_bias

        return grad_input


# 卷积层
class Conv2D:
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, graph=None):
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding
        self.filters = Tensor(initialize_weights((out_channels, in_channels, kernel_size, kernel_size)), requires_grad=True, graph=graph)
        self.graph = graph

    def forward(self, x):
        self.input = x
        self.input_padded = np.pad(x, ((0, 0), (0, 0), (self.padding, self.padding), (self.padding, self.padding)))
        batch_size, in_channels, height, width = self.input_padded.shape
        out_height = (height - self.kernel_size) // self.stride + 1
        out_width = (width - self.kernel_size) // self.stride + 1
        self.out_height, self.out_width = out_height, out_width

        # 使用矩阵乘法实现卷积
        self.columns = self._im2col(self.input_padded)
        self.filters_reshaped = self.filters.data.reshape(self.filters.data.shape[0], -1).T
        output = np.dot(self.columns, self.filters_reshaped)
        output = output.reshape(batch_size, out_height, out_width, -1).transpose(0, 3, 1, 2)
        return Tensor(output, requires_grad=True, graph=self.graph)

    def backward(self, grad_output):
        grad_output_reshaped = grad_output.transpose(0, 2, 3, 1).reshape(-1, grad_output.shape[1])
        grad_filters = np.dot(self.columns.T, grad_output_reshaped)
        grad_filters = grad_filters.transpose(1, 0).reshape(self.filters.data.shape)
        self.filters.grad = grad_filters

        grad_columns = np.dot(grad_output_reshaped, self.filters_reshaped.T)
        grad_input = self._col2im(grad_columns, self.input.shape)
        return grad_input

    def _im2col(self, x):
        batch_size, in_channels, height, width = x.shape
        out_height = (height - self.kernel_size) // self.stride + 1
        out_width = (width - self.kernel_size) // self.stride + 1
        col = np.zeros((batch_size, in_channels, self.kernel_size, self.kernel_size, out_height, out_width))

        for i in range(out_height):
            for j in range(out_width):
                h_start = i * self.stride
                w_start = j * self.stride
                h_end = h_start + self.kernel_size
                w_end = w_start + self.kernel_size
                col[:, :, :, :, i, j] = x[:, :, h_start:h_end, w_start:w_end]

        return col.transpose(0, 4, 5, 1, 2, 3).reshape(-1, in_channels * self.kernel_size * self.kernel_size)

    def _col2im(self, cols, input_shape):
        batch_size, in_channels, height, width = input_shape
        col = cols.reshape(batch_size, self.out_height, self.out_width, in_channels, self.kernel_size, self.kernel_size)
        col = col.transpose(0, 3, 4, 5, 1, 2)
        x = np.zeros((batch_size, in_channels, height, width))
        for y in range(self.kernel_size):
            y_max = y + self.stride * self.out_height
            for x in range(self.kernel_size):
                x_max = x + self.stride * self.out_width
                x[:, :, y:y_max:self.stride, x:x_max:self.stride] += col[:, :, y, x, :, :]
        return x

# 最大池化层
class MaxPooling:
    def __init__(self, pool_size, stride):
        self.pool_size = pool_size
        self.stride = stride

    def forward(self, x):
        self.input = x
        batch_size, channels, height, width = x.shape
        out_height = (height - self.pool_size) // self.stride + 1
        out_width = (width - self.pool_size) // self.stride + 1
        self.out_height, self.out_width = out_height, out_width

        self.indices = np.zeros_like(x)
        output = np.zeros((batch_size, channels, out_height, out_width))

        for i in range(out_height):
            for j in range(out_width):
                h_start = i * self.stride
                w_start = j * self.stride
                h_end = h_start + self.pool_size
                w_end = w_start + self.pool_size
                patch = x[:, :, h_start:h_end, w_start:w_end]
                output[:, :, i, j] = np.max(patch, axis=(2, 3))

                # 记录最大值的位置
                max_indices = patch == np.expand_dims(output[:, :, i, j], axis=(2, 3))
                self.indices[:, :, h_start:h_end, w_start:w_end] += max_indices
        return output

    def backward(self, grad_output):
        grad_input = np.zeros_like(self.input)
        for i in range(self.out_height):
            for j in range(self.out_width):
                h_start = i * self.stride
                w_start = j * self.stride
                h_end = h_start + self.pool_size
                w_end = w_start + self.pool_size
                grad_input[:, :, h_start:h_end, w_start:w_end] += (
                    self.indices[:, :, h_start:h_end, w_start:w_end] * grad_output[:, :, i:i + 1, j:j + 1]
                )
        return grad_input

# 均值池化层
class AveragePooling:
    def __init__(self, pool_size, stride):
        self.pool_size = pool_size
        self.stride = stride

    def forward(self, x):
        batch_size, channels, height, width = x.shape
        out_height = (height - self.pool_size) // self.stride + 1
        out_width = (width - self.pool_size) // self.stride + 1
        output = np.zeros((batch_size, channels, out_height, out_width))

        for i in range(out_height):
            for j in range(out_width):
                h_start = i * self.stride
                w_start = j * self.stride
                h_end = h_start + self.pool_size
                w_end = w_start + self.pool_size
                patch = x[:, :, h_start:h_end, w_start:w_end]
                output[:, :, i, j] = np.mean(patch, axis=(2, 3))
        return output

    def backward(self, grad_output):
        batch_size, channels, height, width = self.input.shape
        grad_input = np.zeros_like(self.input)
        for i in range(self.out_height):
            for j in range(self.out_width):
                h_start = i * self.stride
                w_start = j * self.stride
                h_end = h_start + self.pool_size
                w_end = w_start + self.pool_size
                grad_input[:, :, h_start:h_end, w_start:w_end] += (
                    grad_output[:, :, i:i + 1, j:j + 1] / (self.pool_size * self.pool_size)
                )
        return grad_input

# 全局平均池化层
class GlobalAveragePooling:
    def forward(self, x):
        self.input = x
        return np.mean(x, axis=(2, 3), keepdims=True)

    def backward(self, grad_output):
        batch_size, channels, height, width = self.input.shape
        return np.ones_like(self.input) * grad_output / (height * width)

# 批量归一化层
class BatchNorm:
    def __init__(self, num_features, momentum=0.9):
        self.gamma = Tensor(np.ones(num_features), requires_grad=True)
        self.beta = Tensor(np.zeros(num_features), requires_grad=True)
        self.momentum = momentum
        self.running_mean = np.zeros(num_features)
        self.running_var = np.ones(num_features)

    def forward(self, x, training=True):
        #print("BatchNorm input shape:", x.data.shape)
        if training:
            batch_mean = np.mean(x.data, axis=(0, 2, 3), keepdims=True)
            batch_var = np.var(x.data, axis=(0, 2, 3), keepdims=True)
            self.normalized = (x.data - batch_mean) / np.sqrt(batch_var + 1e-8)

            # 更新滚动均值和方差
            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * batch_mean.squeeze()
            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * batch_var.squeeze()

            gamma = self.gamma.data.reshape(1, -1, 1, 1)
            beta = self.beta.data.reshape(1, -1, 1, 1)
            return Tensor(gamma * self.normalized + beta, requires_grad=True)
        else:
            normalized = (x.data - self.running_mean.reshape(1, -1, 1, 1)) / np.sqrt(self.running_var.reshape(1, -1, 1, 1) + 1e-8)
            gamma = self.gamma.data.reshape(1, -1, 1, 1)
            beta = self.beta.data.reshape(1, -1, 1, 1)
            return Tensor(gamma * normalized + beta, requires_grad=False)

    def backward(self, grad_output):
        grad_normalized = grad_output.data * self.gamma.data.reshape(1, -1, 1, 1)
        grad_var = np.sum(grad_normalized * (self.input.data - self.mean) * -0.5 * (self.variance + 1e-8) ** -1.5, axis=(0, 2, 3), keepdims=True)
        grad_mean = np.sum(grad_normalized * -1 / np.sqrt(self.variance + 1e-8), axis=(0, 2, 3), keepdims=True) + \
                    grad_var * np.mean(-2 * (self.input.data - self.mean), axis=(0, 2, 3), keepdims=True)
        grad_input = grad_normalized / np.sqrt(self.variance + 1e-8) + grad_var * 2 * (self.input.data - self.mean) / self.input.data.size + grad_mean / self.input.data.size
        return grad_input

# 实现Dropout层，用于在训练时随机丢弃一些神经元的输出，推理时保持输出不变
class Dropout:
    def __init__(self, rate=0.5):
        self.rate = rate
        self.mask = None

    def forward(self, input, training=True):
        if training:
            self.mask = np.random.binomial(1, 1 - self.rate, size=input.shape)
            return input * self.mask / (1 - self.rate)  # Scaling during training
        return input  # No dropout during inference

# 定义CNN模型
# 在CNN模型中修改展平后的尺寸
class CNN:
    def __init__(self):
        self.conv1 = Conv2D(1, 8, kernel_size=3, stride=1, padding=0)
        self.bn1 = BatchNorm(8)
        self.pool1 = MaxPooling(pool_size=2, stride=2)
        self.fc1 = FullyConnected(8 * 13 * 13, 10)  # 修正全连接层的输入维度

    def forward(self, x):
        #print("Input shape:", x.shape)  # 打印输入形状，检查是否正确
        x = self.conv1.forward(x)
        #print("After conv1 shape:", x.data.shape)  # 打印卷积层输出形状
        x = self.bn1.forward(x)
        #print("After bn1 shape:", x.data.shape)  # 打印batchnorm层输出形状
        x = relu(x)
        x = self.pool1.forward(x)
        #print("After pool1 shape:", x.shape)  # 打印池化层输出形状
        x = x.reshape(x.shape[0], -1)  # 展平
        #print("After flattening shape:", x.shape)  # 打印展平后的形状
        x = self.fc1.forward(x)
        #print("After fc1 shape:", x.data.shape)  # 打印全连接层输出形状
        return softmax(x)

# 训练与评估
# 训练与评估
def train_in_batches(model, X_train, y_train, X_test, y_test, epochs=5, learning_rate=0.01, batch_size=128):
    graph = Graph()  # 创建计算图
    optimizer = SGD(learning_rate=learning_rate, weight_decay=0.0)

    for epoch in range(epochs):
        indices = np.arange(X_train.shape[0])
        np.random.shuffle(indices)

        epoch_loss = 0
        correct_train = 0
        total_train = 0

        # 批量训练
        for start in range(0, X_train.shape[0], batch_size):
            end = start + batch_size
            batch_indices = indices[start:end]
            batch_images = X_train[batch_indices]
            batch_labels = y_train[batch_indices]

            # 前向传播
            predictions = model.forward(batch_images)
            loss = cross_entropy_loss(batch_labels, predictions.data)
            epoch_loss += loss

            # 计算准确率
            predicted_classes = np.argmax(predictions.data, axis=1)
            true_classes = np.argmax(batch_labels, axis=1)
            correct_train += (predicted_classes == true_classes).sum()
            total_train += batch_labels.shape[0]

            # 后向传播
            grad = predictions.data - batch_labels  # 损失梯度
            model.fc1.backward(grad)

            # 参数更新
            optimizer.step([model.conv1.filters.data, model.fc1.weights.data, model.fc1.bias.data],
                           [model.conv1.filters.grad, model.fc1.d_weights, model.fc1.d_bias])

        # 计算训练集的损失和准确率
        train_accuracy = correct_train / total_train
        print(
            f"Epoch {epoch + 1}/{epochs}, Training Loss: {epoch_loss:.4f}, Training Accuracy: {train_accuracy * 100:.2f}%")

        # 在测试集上评估
        test_predictions = model.forward(X_test)
        test_loss = cross_entropy_loss(y_test, test_predictions.data)
        predicted_classes = np.argmax(test_predictions.data, axis=1)
        true_classes = np.argmax(y_test, axis=1)
        test_accuracy = (predicted_classes == true_classes).mean()

        print(f"Epoch {epoch + 1}/{epochs}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy * 100:.2f}%\n")

        # 推理阶段输出预测结果和预测精度
        print("Test Predictions:", predicted_classes)
        print("Test Accuracy:", test_accuracy * 100, "%")

# 加载数据
train_images = load_mnist_images('./train-images-idx3-ubyte')
train_labels = load_mnist_labels('./train-labels-idx1-ubyte')
test_images = load_mnist_images('./t10k-images-idx3-ubyte')
test_labels = load_mnist_labels('./t10k-labels-idx1-ubyte')

# 独热编码标签
train_labels_onehot = one_hot_encode(train_labels)
test_labels_onehot = one_hot_encode(test_labels)

train_images = train_images[:1000]
train_labels_onehot = train_labels_onehot[:1000]
test_images = test_images[:200]
test_labels_onehot = test_labels_onehot[:200]

model = CNN()
train_in_batches(model, train_images, train_labels_onehot, test_images, test_labels_onehot)
